\documentclass[a4paper,11pt, twocolumn]{article}
\usepackage[pdftex]{graphicx}
% \usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epstopdf}
\usepackage[utf8]{inputenc}
\usepackage{titlesec}
% \usepackage[titletoc]{appendix}
% \titleformat{\chapter}[hang]{\bf\Huge}{\thechapter}{1cm}{}

\usepackage[colorlinks=true]{hyperref}
\hypersetup{urlcolor=blue,linkcolor=black,citecolor=black,colorlinks=true}
\bibliographystyle{plain}

\pagestyle{plain}
% -------------------- this stuff for code --------------------

\usepackage{anysize}
\marginsize{30mm}{30mm}{20mm}{20mm}

\newenvironment{formal}{%
  \def\FrameCommand{%
    \hspace{1pt}%
    {\color{blue}\vrule width 2pt}%
    {\color{formalshade}\vrule width 4pt}%
    \colorbox{formalshade}%
  }%
  \MakeFramed{\advance\hsize-\width\FrameRestore}%
  \noindent\hspace{-4.55pt}% disable indenting first paragraph
  \begin{adjustwidth}{}{7pt}%
  \vspace{2pt}\vspace{2pt}%
}
{%
  \vspace{2pt}\end{adjustwidth}\endMakeFramed%
}

\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}

\usepackage{color}
\usepackage{dsfont}
\usepackage[bitstream-charter]{mathdesign}
\usepackage[scaled]{helvet}
\usepackage{inconsolata}


\definecolor{colKeys}{rgb}{0,0,0.9} 
\definecolor{colIdentifier}{rgb}{0,0,0} 
\definecolor{colString}{rgb}{0.7,0,0} 
\definecolor{colComments}{rgb}{0,0.6,0} 
\usepackage{listings}
\lstset{
  stringstyle=\color{colString},
  keywordstyle=\color{colKeys},
  identifierstyle=\color{colIdentifier},
  commentstyle=\color{colComments},
  numbers=left,
  tabsize=4,
  frame=single,
  breaklines=true,
  basicstyle=\small\ttfamily,
  numberstyle=\tiny\ttfamily,
  framexleftmargin=0mm,
  xleftmargin=7mm,
  xrightmargin=7mm,
  frameround={tttt},
  captionpos=b
}

\usepackage{mathtools}
\usepackage{amsthm}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\DeclareMathOperator*{\argmin}{ArgMin\ }
\DeclareMathOperator*{\argmax}{ArgMax\ }

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[usenames,dvipsnames]{xcolor}
\makeatletter
\DeclareRobustCommand{\em}{%
  \@nomath\em \if b\expandafter\@car\f@series\@nil
  \normalfont \else \bfseries \fi}
\makeatother

%% Headers and footers
\usepackage{fancyhdr}
\usepackage[section]{placeins}
\pagestyle{fancy}
\fancyhf{}
\addtolength{\headwidth}{30pt}
\addtolength{\headwidth}{30pt}
\renewcommand{\headrulewidth}{0.4pt} % thickness of the header line
\renewcommand{\footrulewidth}{0.4pt} % thickness of the footer line
% \renewcommand{\chaptermark}[1]{\markboth{#1}{#1}} % chapter name
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}  % section name
\lhead[\fancyplain{}{\bf\thepage}]{\fancyplain{}{\bf\rightmark}} % display header
\rhead[\fancyplain{}{\bf\leftmark}]{\fancyplain{}{}} % display header
\fancyfoot[C]{\bf\thepage} % display footer (page number)
\fancyfoot[R]{\bf\today} % display footer (date)
\fancypagestyle{plain}{ 
	\fancyhead{} \renewcommand{\headrulewidth}{0pt}
}
% \newcommand{\clearemptydoublepage}{\newpage{\pagestyle{plain}\cleardoublepage}}

\usepackage[T1]{fontenc}
\usepackage{enumerate}
\usepackage{afterpage,lastpage,fancyhdr}
\usepackage[includeheadfoot,margin=2.5cm]{geometry}
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 

% \DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

% \makeatletter \def\thickhrulefill{\leavevmode \leaders \hrule height 1pt\hfill
% \kern \z@} \renewcommand{\maketitle}{
%     \begin{titlepage}
%     \let\footnotesize\small \let\footnoterule\relax \parindent \z@ \reset@font
%     \null\vfil
%     \vspace{-20mm}
%     \begin{center}
%     {\small \scshape Imperial College London \\ Department of Computing}
%     \end{center}
%     \vspace{0.5cm}
% 	\begin{minipage}{\textwidth}
% 		\vspace{1cm}
% 		\noindent\rule[0ex]{\textwidth}{4pt} \\
% 		\flushright
% 		\center
% 		\@title
% 		\\ \vspace{4mm}
% 		\noindent\rule[0ex]{\textwidth}{4pt} \\
% 	\end{minipage}
% 	\vspace{1.5cm}
% 	\begin{minipage}{\textwidth}
% 		\flushright
% 		{\bfseries}
% 		\vspace{7mm}
% 		\flushleft
% 		\@author.\\
% 	\end{minipage}
% 	\vspace{0.5cm}
% 	\begin{center}
% 		\includegraphics[width=70mm,]{pictures/logo_imperial_college_london.png}
% 	\end{center}
% 	\vspace{\stretch{1}}
% 	\vspace{50mm}
% 		\flushleft
% 		{\bfseries}
% 		{\small \scshape \@date }.
% 		\vspace{0.1cm}
% 		\rule{\linewidth}{.5pt}
%   \end{titlepage}
%   \setcounter{footnote}{1}
%   \setcounter{page}{2}
% }


\author{
    Ben Graham (bh110, c3), \\
    Paul Gribelyuk (pg1312, a5)
}
\date{\today}

\title{The ``Smooth'' Challenge}



\begin{document}
\maketitle
\section{Introduction}
The ``Smooth'' program is a mesh pre-processing algorithm responsible for improving a specific, measurable characteristic, known as \emph{quality}.  A higher \emph{quality} mesh aids local convergence properties in finite element analysis, where non-uniform domains require varying degrees of granularity.  The ``Smooth'' algorithm concerns a a triangular disretization in 2D. The \emph{quality} of the individual triangle faces based on a nonlinear combination of the components of a metric tensor at each of the nodes of that triangle.  In our simulations, a suboptimally constructed mesh typically sees an average improvement of 6\% with the minimum \emph{quality} of any triangular face increasing 30-35\%.  We investigated various performance enhancements on the \textbf{Intel Xeon CPU E5606 2.13GHz} CPU with 4 cores.  Our approach was twofold.  We first conisdered serial optimisations by using various profiling tools to minimize instruction count and by exploiting locality of some of the data.   Next, we exploited the inherent parallelism using the OpenCL programming on the \textbf{NVIDIA GE Force 570 (GF110 architecture)}, available through the computer lab on the ``Graphics'' machines.  OpenCL version 1.1 was used for the GPU code, while the GCC 4.6.3 compiler compiled C++ code.  All recorded benchmark results were obtained with the -O3 optimisation flag.

\section{Profiling the Code}
We first tackled the problem of optimising the serial version of the code.  Optimisations at this stage will also help in the parallel implementation, especially when those performance gains are seen in the parallel region of the code (generally speaking).  An Apple MacBook Air (with Intel i7 2.0GHz) with XCode, GCC 4.2 and Instruments (for profiling) was initially used.  This process identifield that the code spending 60\% of the time in \verb+element_quality+, which is used to evaluate the quantitative effect of local changes to node coordinates within a mesh.  For a node neighboring $n$ triangles, $2n$ calls are made to this function.  Furthermore, if $N$ nodes exist in the mesh and we use $I$ iterations to converge to a more optimal mesh, the total number of these calls is bounded by:
$$
2N\cdot n\Delta(M)
$$
where $\Delta(M)$ is the maximum degree of the mesh.  Within \verb+element_quality+, the standard power function $pow(x, y) = x^y$ took an inordinate amount of time, as well as computations for \verb+element_area+ and accesses to elements of \verb+vector+ fields in the \verb+Mesh+ class.  Some further time was spent solving a 2-by-2 system of linear equations and in helper functions measuring properties of the mesh.  The profiler allowed us to further determine that these resource uses were far from optimal since processor usage varied greatly throughout the program's execution.  As expected the program does not tax the memory system, given that the only updates are to two floating point values in the form of coorodinate updates.

\section{Optimizing in Serially}
The \verb+Mesh+ class uses \verb+std::vector+ objects to encapsulate relationships between Nodes and mesh Elements.  However, adjacent Nodes have coordinates and metric values in spacially disparate places.  However, we will tackle this challenge on the GPU by passing vectorized arrays to the device.  
We gained an initial speedup by re-writing the \verb+pow()+ call explicitly as 3 multiplications, which showed decrease in time from 7.05 seconds to 6.72 (4.7\% improvement).  Next, the branching code for the variable \verb+f = min(l/3.0, 3.0/l)+ to eliminate the division, thus saving clock cycles and reducing the mis-prediction penalty at that stage.  Although earlier in the \verb+element_quality+ function, the call to \verb+element_area+ seems to retrieve the same data elements already retrieved in the calling method, combining the functions showed no effect, since, we believe, the compiler is already making this optimisation for us.  
\section{Parallelising using OpenCL}
As first, naiive, parallelisation attempt, we used OMP to distribute work among the cores of the CPU.  Although the loop controlling convergence of the algorithm could not be parallelised (since prior iterations had changed the mesh to an incrementally higher quality), the next loop, over the Nodes assigned nodes available cores.  Although this saw a factor 2 speedup and produced almost identical results to the serial version, it opens the door for the possibility of adjacent Nodes being altered (or worse, the same Node) by parallel threads of execution, leaving a lower quality grid than previously found.  When we inserted critical regions around all nodes neighboring each worker's Node, but saw a significant decrease in performance as threads stalled waiting for other threads to release their data locks.
\section{Results}
%  various charts and tables of data from running the code
\section{Conclusion}


% reordering the nodes to keep all threads working, means eliminiating spacial locality didn't work


\end{document}  
